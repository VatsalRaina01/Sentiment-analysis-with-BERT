{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "runtime_attributes": {
        "runtime_version": "2025.07"
      },
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPkrRALQprpsD+Bcx6ay0bc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VatsalRaina01/Sentiment-analysis-with-BERT/blob/main/sentimentanalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ck0KpqKV22n9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.offline as pyo\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Download dataset\n",
        "current_folder = os.getcwd()\n",
        "dataset = tf.keras.utils.get_file(\n",
        "    fname=\"aclImdb.tar.gz\",\n",
        "    origin=\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
        "    cache_dir=current_folder,\n",
        "    extract=True\n",
        ")\n",
        "\n",
        "dataset_path = os.path.dirname(dataset)\n",
        "dataset_dir = os.path.join(dataset_path, 'aclImdb')\n",
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "test_dir = os.path.join(dataset_dir, 'test')\n",
        "\n",
        "# Load dataset\n",
        "def load_dataset(directory):\n",
        "    data = {\"sentence\": [], \"sentiment\": []}\n",
        "    for file_name in os.listdir(directory):\n",
        "        if file_name == 'pos':\n",
        "            for text_file in os.listdir(os.path.join(directory, file_name)):\n",
        "                with open(os.path.join(directory, file_name, text_file), \"r\", encoding=\"utf-8\") as f:\n",
        "                    data[\"sentence\"].append(f.read())\n",
        "                    data[\"sentiment\"].append(1)\n",
        "        elif file_name == 'neg':\n",
        "            for text_file in os.listdir(os.path.join(directory, file_name)):\n",
        "                with open(os.path.join(directory, file_name, text_file), \"r\", encoding=\"utf-8\") as f:\n",
        "                    data[\"sentence\"].append(f.read())\n",
        "                    data[\"sentiment\"].append(0)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "train_df = load_dataset(train_dir)\n",
        "test_df = load_dataset(test_dir)\n",
        "\n",
        "# Plot sentiment counts\n",
        "sentiment_counts = train_df['sentiment'].value_counts()\n",
        "fig = px.bar(\n",
        "    x={0: 'Negative', 1: 'Positive'},\n",
        "    y=sentiment_counts.values,\n",
        "    color=sentiment_counts.index,\n",
        "    color_discrete_sequence=px.colors.qualitative.Dark24,\n",
        "    title='<b>Sentiment Counts'\n",
        ")\n",
        "fig.update_layout(template='plotly_dark')\n",
        "pyo.plot(fig, filename='Sentiments_Counts.html', auto_open=True)\n",
        "\n",
        "# Clean text\n",
        "def text_cleaning(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    text = re.sub(r'\\[[^]]*\\]', '', soup.get_text())\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s,']\", '', text)\n",
        "    return text\n",
        "\n",
        "train_df['Cleaned_sentence'] = train_df['sentence'].apply(text_cleaning)\n",
        "test_df['Cleaned_sentence'] = test_df['sentence'].apply(text_cleaning)\n",
        "\n",
        "# Word clouds\n",
        "def generate_wordcloud(text, title):\n",
        "    all_text = \" \".join(text)\n",
        "    wordcloud = WordCloud(width=800, height=400, stopwords=set(STOPWORDS), background_color='black').generate(all_text)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "generate_wordcloud(train_df[train_df['sentiment']==1]['Cleaned_sentence'], 'Positive Review')\n",
        "generate_wordcloud(train_df[train_df['sentiment']==0]['Cleaned_sentence'], 'Negative Review')\n",
        "\n",
        "# Prepare train/val/test\n",
        "Reviews = train_df['Cleaned_sentence']\n",
        "Target = train_df['sentiment']\n",
        "test_reviews = test_df['Cleaned_sentence']\n",
        "test_targets = test_df['sentiment']\n",
        "\n",
        "x_val, x_test, y_val, y_test = train_test_split(test_reviews, test_targets, test_size=0.5, stratify=test_targets)\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
        "\n",
        "max_len = 128\n",
        "X_train_encoded = tokenizer.batch_encode_plus(Reviews.tolist(), padding=True, truncation=True, max_length=max_len, return_tensors='tf')\n",
        "X_val_encoded = tokenizer.batch_encode_plus(x_val.tolist(), padding=True, truncation=True, max_length=max_len, return_tensors='tf')\n",
        "X_test_encoded = tokenizer.batch_encode_plus(x_test.tolist(), padding=True, truncation=True, max_length=max_len, return_tensors='tf')\n",
        "\n",
        "# Model (TF-ready)\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-uncased-finetuned-sst-2-english',\n",
        "    num_labels=2,\n",
        "    from_pt=False\n",
        ")\n",
        "\n",
        "# Compile\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "# Train\n",
        "history = model.fit(\n",
        "    [X_train_encoded['input_ids'], X_train_encoded['attention_mask']],\n",
        "    Target,\n",
        "    validation_data=([X_val_encoded['input_ids'], X_val_encoded['attention_mask']], y_val),\n",
        "    batch_size=32,\n",
        "    epochs=2\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_accuracy = model.evaluate(\n",
        "    [X_test_encoded['input_ids'], X_test_encoded['attention_mask']],\n",
        "    y_test\n",
        ")\n",
        "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')\n",
        "\n",
        "# Save\n",
        "path = '/content'\n",
        "tokenizer.save_pretrained(path + '/Tokenizer')\n",
        "model.save_pretrained(path + '/Model')\n",
        "\n",
        "# Load back\n",
        "distil_tokenizer = DistilBertTokenizer.from_pretrained(path + '/Tokenizer')\n",
        "distil_model = TFDistilBertForSequenceClassification.from_pretrained(path + '/Model', from_pt=False)\n",
        "\n",
        "# Prediction example\n",
        "def Get_sentiment(Review, Tokenizer=distil_tokenizer, Model=distil_model):\n",
        "    if not isinstance(Review, list):\n",
        "        Review = [Review]\n",
        "    encoded = Tokenizer.batch_encode_plus(Review, padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
        "    prediction = Model.predict([encoded['input_ids'], encoded['attention_mask']])\n",
        "    label_map = {1: 'positive', 0: 'negative'}\n",
        "    pred_labels = tf.argmax(prediction.logits, axis=1).numpy().tolist()\n",
        "    return [label_map[i] for i in pred_labels]\n",
        "\n",
        "Review = \"\"\"Bahubali is a blockbuster Indian movie that was released in 2015.\n",
        "It is the first part of a two-part epic saga that tells the story of a legendary hero who fights for his kingdom and his love.\n",
        "The movie has received rave reviews from critics and audiences alike for its stunning visuals, spectacular action scenes, and captivating storyline.\"\"\"\n",
        "print(Get_sentiment(Review))\n"
      ]
    }
  ]
}